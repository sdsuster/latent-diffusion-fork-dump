{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def init_random_2d_freqs(head_dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "    freqs_x = []\n",
    "    freqs_y = []\n",
    "    theta = theta\n",
    "    mag = 1 / (theta ** (torch.arange(0, head_dim, 4)[: (head_dim // 4)].float() / head_dim))\n",
    "    for i in range(num_heads):\n",
    "        angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
    "        fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi/2 + angles)], dim=-1)\n",
    "        fy = torch.cat([mag * torch.sin(angles + torch.pi/12), mag * torch.sin(torch.pi/2 + angles)], dim=-1)\n",
    "        freqs_x.append(fx)\n",
    "        freqs_y.append(fy)\n",
    "    freqs_x = torch.stack(freqs_x, dim=0)\n",
    "    freqs_y = torch.stack(freqs_y, dim=0)\n",
    "    freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
    "    return freqs\n",
    "\n",
    "def init_random_3d_freqs(head_dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "    \"\"\"\n",
    "    Initialize frequency parameters for 3D rotary embeddings.\n",
    "    \n",
    "    Assumes that head_dim is divisible by 6 so that the rotary sub-dimension per axis is (head_dim // 6)*2,\n",
    "    and that we want three sets of phase shifts per axis.\n",
    "    \n",
    "    For each axis, we generate three phase shifts:\n",
    "      - 0 offset:      cos(angle)\n",
    "      - π/2 offset:    cos(π/2 + angle)\n",
    "      - π offset:      cos(π + angle)\n",
    "    Adjust these offsets as needed.\n",
    "    \"\"\"\n",
    "    freqs_x, freqs_y, freqs_z = [], [], []\n",
    "    # For each axis, we want three groups, so determine the number of frequency pairs per group.\n",
    "    num_pairs = head_dim // 6  # each pair (cos, sin) becomes one complex number\n",
    "    \n",
    "    # Create a magnitude vector of length num_pairs\n",
    "    mag = 1 / (theta ** (torch.arange(num_pairs, dtype=torch.float32) / num_pairs))\n",
    "    \n",
    "    for _ in range(num_heads):\n",
    "        # Generate axis-specific random angles (or zeros if rotation is disabled)\n",
    "        angle_x = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
    "        angle_y = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
    "        angle_z = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)\n",
    "        \n",
    "        # For each axis, create three sets of frequency components.\n",
    "        fx = torch.cat([\n",
    "            mag * torch.cos(angle_x),\n",
    "            mag * torch.cos(torch.pi/2 + angle_x),\n",
    "            mag * torch.cos(torch.pi + angle_x)\n",
    "        ], dim=-1)\n",
    "        fy = torch.cat([\n",
    "            mag * torch.cos(angle_y),\n",
    "            mag * torch.cos(torch.pi/2 + angle_y),\n",
    "            mag * torch.cos(torch.pi + angle_y)\n",
    "        ], dim=-1)\n",
    "        fz = torch.cat([\n",
    "            mag * torch.cos(angle_z),\n",
    "            mag * torch.cos(torch.pi/2 + angle_z),\n",
    "            mag * torch.cos(torch.pi + angle_z)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        freqs_x.append(fx)\n",
    "        freqs_y.append(fy)\n",
    "        freqs_z.append(fz)\n",
    "    \n",
    "    # Stack into a frequency tensor with shape [3, num_heads, 3*num_pairs]\n",
    "    freqs_x = torch.stack(freqs_x, dim=0)\n",
    "    freqs_y = torch.stack(freqs_y, dim=0)\n",
    "    freqs_z = torch.stack(freqs_z, dim=0)\n",
    "    freqs = torch.stack([freqs_x, freqs_y, freqs_z], dim=0)\n",
    "    \n",
    "    return freqs\n",
    "\n",
    "def compute_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, t_z: torch.Tensor = None):\n",
    "    N = t_x.shape[0]\n",
    "    # No float 16 for this range\n",
    "    with torch.amp.autocast('cuda', enabled=False):\n",
    "        freqs_x = (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2))\n",
    "        freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2))\n",
    "        if t_z != None:\n",
    "            freqs_z = (t_z.unsqueeze(-1) @ freqs[2].unsqueeze(-2))\n",
    "            freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y + freqs_z)\n",
    "            \n",
    "            a = (freqs_x + freqs_y + freqs_z)\n",
    "            x = freqs_x.flatten()\n",
    "            y = freqs_y.flatten()\n",
    "            z = freqs_z.flatten()\n",
    "            # for i in range(a.shape[1]):\n",
    "            #     # print([x[i], y[i], z[i]])\n",
    "            #     print('B',a[0, i, :])\n",
    "        else:\n",
    "            a = (freqs_x + freqs_y)\n",
    "            x = freqs_x.flatten()\n",
    "            y = freqs_y.flatten()\n",
    "            # for i in range(a.shape[1]):\n",
    "            #     # print([x[i], y[i], z[i]])\n",
    "            #     print('B',a[0, i, :])\n",
    "            freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
    "\n",
    "    return freqs_cis\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int, zero_center=False):\n",
    "    t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = torch.div(t, end_x, rounding_mode='floor').float()\n",
    "    \n",
    "    return t_x, t_y\n",
    "\n",
    "def init_t_xyz(end_x: int, end_y: int, end_z: int, zero_center=False):\n",
    "    t = torch.arange(end_x * end_y * end_z, dtype=torch.float32)\n",
    "    t_x = (t % end_x).float()\n",
    "    t_y = ((t // end_x) % end_y).float()  # Compute y-axis\n",
    "    t_z = (t // (end_x * end_y)).float()  # Compute z-axis\n",
    "    return t_x, t_y, t_z\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    print(freqs_cis.shape, x.shape)\n",
    "    if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-2 else 1 for i, d in enumerate(x.shape)]\n",
    "    elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-3 else 1 for i, d in enumerate(x.shape)]\n",
    "    elif freqs_cis.shape == (x.shape[-4], x.shape[-3], x.shape[-2], x.shape[-1]):\n",
    "        shape = [d if i >= ndim-4 else 1 for i, d in enumerate(x.shape)]\n",
    "    else:\n",
    "        raise ValueError(\"freqs_cis shape does not match expected dimensions.\")\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    "):\n",
    "    with torch.amp.autocast('cuda', enabled=False):\n",
    "        xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "        xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "        print('B',xq_.shape)\n",
    "        freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "        xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
    "        xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
    "    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the model checkpoint\n",
    "checkpoint = torch.load('./logs/2025-02-27T02-14-17_swlin_unetr_btcv_no_rope/checkpoints/last.ckpt')\n",
    "# # Extract the state_dict\n",
    "if 'state_dict' in checkpoint:    \n",
    "    state_dict = checkpoint['state_dict']\n",
    "else:    \n",
    "    state_dict = checkpoint# Save\n",
    "\n",
    "def print_nested_keys(d, parent_key=\"\"):\n",
    "    \"\"\" Recursively print nested keys in a checkpoint \"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        for key in d:\n",
    "            full_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "            print(full_key)\n",
    "            print_nested_keys(d[key], full_key)\n",
    "    elif isinstance(d, list):\n",
    "        print(f\"{parent_key} -> List of length {len(d)}\")\n",
    "    else:\n",
    "        print(f\"{parent_key} -> {type(d)}\")\n",
    "\n",
    "print(\"Checkpoint Structure:\")\n",
    "print_nested_keys(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dice_Necrotic': 89.47, 'Dice_Edema': 91.14, 'Dice_Enhancing': 86.88, 'Dice_WT': 89.16333333333334, 'Dice_TC': 88.175, 'Dice_ET': 86.88}\n"
     ]
    }
   ],
   "source": [
    "def compute_brats_metrics(dice_scores):\n",
    "    \"\"\"\n",
    "    Compute BraTS segmentation metrics: Whole Tumor (WT), Tumor Core (TC), Enhancing Tumor (ET).\n",
    "\n",
    "    Args:\n",
    "        dice_scores (list or tensor): List of Dice scores for each tumor class.\n",
    "            - dice_scores[0] -> Dice_Necrotic (NCR)\n",
    "            - dice_scores[1] -> Dice_Edema (ED)\n",
    "            - dice_scores[2] -> Dice_Enhancing Tumor (ET)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing BraTS metrics:\n",
    "              - Dice_WT (Whole Tumor)\n",
    "              - Dice_TC (Tumor Core)\n",
    "              - Dice_ET (Enhancing Tumor)\n",
    "    \"\"\"\n",
    "    Dice_Necrotic = dice_scores[0]\n",
    "    Dice_Edema = dice_scores[1]\n",
    "    Dice_Enhancing = dice_scores[2]\n",
    "\n",
    "    # Compute BraTS Dice metrics\n",
    "    Dice_WT = (Dice_Necrotic + Dice_Edema + Dice_Enhancing) / 3  # Whole Tumor\n",
    "    Dice_TC = (Dice_Necrotic + Dice_Enhancing) / 2  # Tumor Core\n",
    "    Dice_ET = Dice_Enhancing  # Enhancing Tumor\n",
    "\n",
    "    return {\n",
    "        \"Dice_Necrotic\": Dice_Necrotic,\n",
    "        \"Dice_Edema\": Dice_Edema,\n",
    "        \"Dice_Enhancing\": Dice_Enhancing,\n",
    "        \"Dice_WT\": Dice_WT,\n",
    "        \"Dice_TC\": Dice_TC,\n",
    "        \"Dice_ET\": Dice_ET,\n",
    "    }\n",
    "\n",
    "print(compute_brats_metrics([89.47, 91.14, 86.88]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = 12  # Dimensionality per head\n",
    "num_heads = 2  # Number of attention heads\n",
    "end_x, end_y, end_z = 4, 4, 4 # Grid size\n",
    "batch_size = 2  # Example batch size\n",
    "seq_len = end_x * end_y * end_z # Sequence length (tokens in spatial grid)\n",
    "\n",
    "# Step 2: Initialize 2D rotary frequencies\n",
    "freqs = init_random_3d_freqs(head_dim, num_heads, theta=100)\n",
    "# Step 3: Generate token positions\n",
    "t_x, t_y, t_z = init_t_xyz(end_x, end_y, end_z)\n",
    "\n",
    "# Step 4: Compute complex frequency embeddings\n",
    "freqs_cis = compute_cis(freqs, t_x, t_y, t_z)\n",
    "# print(freqs.shape)\n",
    "\n",
    "# Step 5: Generate random query and key tensors\n",
    "# Shape: (batch, seq_len, num_heads, head_dim)\n",
    "xq = torch.ones(batch_size, num_heads, seq_len, head_dim, dtype=torch.float32)\n",
    "xk = torch.ones(batch_size, num_heads, seq_len, head_dim, dtype=torch.float32)\n",
    "\n",
    "print(xq.shape, xk.shape, freqs_cis.shape)\n",
    "# Step 6: Apply rotary embeddings\n",
    "xq_out, xk_out = apply_rotary_emb(xq.to(torch.bfloat16), xk.to(torch.bfloat16), freqs_cis)\n",
    "print(xq_out, xk_out)\n",
    "A = torch.softmax(xq_out @ xk_out.transpose(-1, -2), -1)\n",
    "# for i in range(seq_len):\n",
    "#     print('A', A[0, 0, i, :])\n",
    "\n",
    "orig_norm = torch.norm(xq.to(torch.bfloat16), dim=-1)\n",
    "rot_norm = torch.norm(xq_out, dim=-1)\n",
    "print(\"Original norm stats: \", orig_norm.mean().item(), orig_norm.std().item())\n",
    "print(\"Rotated norm stats: \", rot_norm.mean().item(), rot_norm.std().item())\n",
    "print(\"xq_out sample:\", xq_out[0,0,1,:])\n",
    "print(\"xk_out sample:\", xk_out[0,0,1,:])\n",
    "row_sums = A.sum(dim=-1)\n",
    "print(\"Row sums (should be 1):\", row_sums)\n",
    "# Step 7: Print results\n",
    "print(f\"Original xq shape: {xq.shape}, xq_out shape: {xq_out.shape}\")\n",
    "print(f\"Original xk shape: {xk.shape}, xk_out shape: {xk_out.shape}\")\n",
    "\n",
    "# Check if output shapes match input shapes\n",
    "assert xq_out.shape == xq.shape, \"xq_out shape mismatch!\"\n",
    "assert xk_out.shape == xk.shape, \"xk_out shape mismatch!\"\n",
    "\n",
    "print(\"✅ Pipeline executed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = 4  # Dimensionality per head\n",
    "num_heads = 2  # Number of attention heads\n",
    "end_x, end_y = 2, 2 # Grid size\n",
    "batch_size = 1  # Example batch size\n",
    "seq_len = end_x * end_y # Sequence length (tokens in spatial grid)\n",
    "\n",
    "# Step 2: Initialize 2D rotary frequencies\n",
    "freqs = init_random_2d_freqs(head_dim, num_heads, theta=100)\n",
    "# Step 3: Generate token positions\n",
    "t_x, t_y= init_t_xy(end_x, end_y)\n",
    "\n",
    "# Step 4: Compute complex frequency embeddings\n",
    "freqs_cis = compute_cis(freqs, t_x, t_y)\n",
    "print(freqs.shape)\n",
    "\n",
    "# Step 5: Generate random query and key tensors\n",
    "# Shape: (batch, seq_len, num_heads, head_dim)\n",
    "xq = torch.ones(batch_size, num_heads, seq_len, head_dim, dtype=torch.float32)\n",
    "xk = torch.ones(batch_size, num_heads, seq_len, head_dim, dtype=torch.float32)\n",
    "\n",
    "print(xq.shape, xk.shape, freqs_cis.shape)\n",
    "# Step 6: Apply rotary embeddings\n",
    "xq_out, xk_out = apply_rotary_emb(xq.to(torch.bfloat16), xk.to(torch.bfloat16), freqs_cis.to(torch.bfloat16))\n",
    "print(xq_out.shape, xk_out.shape, xq.shape, xk.shape)\n",
    "A = torch.softmax(xq_out @ xk_out.transpose(-1, -2), -1)\n",
    "for i in range(seq_len):\n",
    "    print('A', A[0, 0, i, :])\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Original xq shape: {xq.shape}, xq_out shape: {xq_out.shape}\")\n",
    "print(f\"Original xk shape: {xk.shape}, xk_out shape: {xk_out.shape}\")\n",
    "\n",
    "# Check if output shapes match input shapes\n",
    "assert xq_out.shape == xq.shape, \"xq_out shape mismatch!\"\n",
    "assert xk_out.shape == xk.shape, \"xk_out shape mismatch!\"\n",
    "\n",
    "print(\"✅ Pipeline executed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
